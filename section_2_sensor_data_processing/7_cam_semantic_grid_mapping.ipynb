{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header Image]( ../assets/header_image.png \"Header Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Assignment: Camera-based Semantic Grid Mapping__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the assignment **Camera-based Semantic Grid Mapping**.\n",
    "\n",
    "In this assignment, we will use images taken from multiple vehicle-mounted cameras to obtain a **360Â° bird's eye view (BEV)** of the road.  Our approach will be based on **inverse perspective mapping (IPM)**.\n",
    "\n",
    "Then, we will use IPM on semantically segmented images to get a **semantic grid map** of the vehicle's surroundings, as shown in the picture below.\n",
    "\n",
    "The semantic grid maps are similar to occupancy grid maps and give information about what kind of objects are on the road in a top-down view.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example](ipm_assets/images/demo_carla.png \"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic grid map we will obtain makes the tasks of planning and prediction easier.\n",
    "\n",
    "In this assignment we will walk through the following steps:\n",
    "\n",
    "- Load images, and the camera parameters. \n",
    "- Use OpenCV to apply the inverse perspective mapping.\n",
    "- Use the pinhole camera model.\n",
    "- Apply coordinate system transformations.\n",
    "- Perform IPM.\n",
    "- Stitch multiple images in BEV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing all the necessary packages for this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title=None):\n",
    "    \"\"\"this function shows an opencv image in this notebook\"\"\"\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(rgb_img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the Inputs\n",
    "\n",
    "We will start this assignment by applying **IPM using RGB images** (not segmented) captured by vehicle-mounted cameras in a simulator (VTD). The image loaded by the following code segment was captured by the front right camera of the vehicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "img = cv2.imread(\"ipm_assets/images/vr_1.png\")\n",
    "# show image \n",
    "show_image(img, \"Original Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Task: Use OpenCV to perform IPM__\n",
    "In this task, you will learn how to use OpenCV to transform an image taken by a camera into a BEV image. As shown in the image below. \n",
    "\n",
    "![Expected Output](ipm_assets/images/expected_output_warp_persp.png \"Expected Output\")\n",
    "\n",
    "This mapping can be achieved using a homogeneous transformation **P** (which we will call `P_cam_to_bev` in our code).\n",
    "This transformation can map a point, in camera-image coordinates, to road coordinates (BEV coordinates).\n",
    "\n",
    "Using the **homogeneous** coordinates **x<sub>c<sub>i</sub></sub>** for the camera image and **x<sub>r<sub>i</sub></sub>** for the road, we can map points **x<sub>c<sub>i</sub></sub>** to points **x<sub>r<sub>i</sub></sub>** using a matrix multiplication.\n",
    "\n",
    "\n",
    "The homogenous transformation **P** is a **projective transformation** and can be written as a **3x3 matrix**:\n",
    "\n",
    "$$ \\large P =  \\begin{bmatrix} p_{11} & p_{12} & p_{13} \\\\ p_{21} & p_{22} & p_{23}\\\\p_{31} & p_{32} & 1 \\end{bmatrix}$$\n",
    "\n",
    "Points in images coordinates get mapped to road coordinates as follows:\n",
    "\n",
    "$$ \\large \\begin{bmatrix} w_i x_{r_i} \\\\ w_i y_{r_i} \\\\ w_i \\end{bmatrix} = P \\begin{bmatrix} x_{c_i} \\\\ y_{c_i} \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Using the equation above and by knowing **four source points in the image** coordinates and their **corresponding four points in the road coordiantes** we can solve a system of equations for the 8 unknown parameters of **P**. \n",
    "\n",
    "Thankfully OpenCV provides a method to do this!\n",
    "\n",
    "Using `cv2.getPerspectiveTransform()`, which takes `source_coordinates` (**x<sub>c<sub>1</sub></sub>, x<sub>c<sub>2</sub></sub>, x<sub>c<sub>3</sub></sub>, x<sub>c<sub>4</sub></sub>**) and `destination_coordinates` (**x<sub>r<sub>1</sub></sub>, x<sub>r<sub>2</sub></sub>, x<sub>r<sub>3</sub></sub>, x<sub>r<sub>4</sub></sub>**) as parameters, we can calculate the matrix of a transformation from the provided source points to the destination points. Internally this function applies gaussian elimination (default, other methods found [here](https://docs.opencv.org/master/d2/de8/group__core__array.html#gaaf9ea5dcc392d5ae04eacb9920b9674c) to calculate the elements of the perspective mapping matrix. \n",
    "\n",
    "Your task is to provide 4 source and 4 destinations point for `getPerspectiveTransform()`. \n",
    "The source points sould be provided in image coordinates (x<sub>c<sub>i</sub></sub>,y<sub>c<sub>i</sub></sub>).\n",
    "\n",
    "A practical way to achieve this is by choosing the points in a way that they form a rectangle in the top-down view.\n",
    "\n",
    "Then you will apply the resulting transformation matrix to produce a BEV image using `cv2.warpPerpective()`. \n",
    "\n",
    "Replace the `None` placeholders with your code.\n",
    "\n",
    "#### __Hints__:\n",
    "- The source and destination points need to be provided as py python lists\n",
    "- (Only for **local usage**, doesn't work for JupyterHub) `cv2.show()` is used to plot the input image. Hover over the image. The current position of the mouse in image coordinates is plotted on the bottom left corner. Use this to get the source points coordinates. You can increase the time in `cv2.waitKey()` (specified in milliseconds) which sets for how long the plot is shown. Use `cv2.getPerspectiveTransform()` to calculate the inverse prspective mapping matrix. Refer to the [documentation](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae) for more details.\n",
    "- Use `cv2.warpPerpective()` to apply the calculated perspective mapping matrix on the input image. Refer to the [documentation](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of output image\n",
    "height, width = 600, 400\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# plot the input image\n",
    "#cv2.imshow(\"Input image\", img)\n",
    "#cv2.waitKey(60)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "# define the source points\n",
    "x_c_1, x_c_2, x_c_3, x_c_4 = None, None, None, None\n",
    "\n",
    "src_pts = np.float32([x_c_1, x_c_2, x_c_3, x_c_4])\n",
    "# define the destination points \n",
    "x_r_1, x_r_2, x_r_3, x_r_4 = None, None, None, None\n",
    "\n",
    "dst_pts =np.float32([x_r_1, x_r_2, x_r_3, x_r_4])\n",
    "\n",
    "# calculated the perspective transform matrix\n",
    "P_cam_to_bev = None\n",
    "#print (P_cam_to_bev)\n",
    "\n",
    "# caculate the output image\n",
    "output = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "show_image(output, \"Output Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will draw a trapezoid connecting the source points in the image and perform the calculated transformation on the image again to see how the trapezoid is mapped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw a line\n",
    "def draw_line(image, pt1, pt2, color=(0,0,255), thickness=5):\n",
    "    print(pt1, pt2)\n",
    "    cv2.line(image, pt1.astype(int), pt2.astype(int), color, thickness)\n",
    "    \n",
    "# function to draw rectanges:\n",
    "def draw_rectange(image, pts, color=(0,0,255), thickness=5):\n",
    "    draw_line(image, pts[0], pts[1], color, thickness)\n",
    "    draw_line(image, pts[1], pts[2], color, thickness)\n",
    "    draw_line(image, pts[2], pts[3], color, thickness)\n",
    "    draw_line(image, pts[3], pts[0], color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a rectange that shows the source point\n",
    "draw_rectange(img, src_pts)\n",
    "# draw a rectangle that shows the destination points\n",
    "#draw_rectange(output, dst_pts)\n",
    "# caculate the output image\n",
    "output =cv2.warpPerspective(img ,P_cam_to_bev, (width,height))\n",
    "\n",
    "\n",
    "show_image(img, \"Input Image\")\n",
    "show_image(output, \"Output Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method doesn't only work for mapping images into a BEV. It can map any view into another, as long as the source and destination points are coplanar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __IPM using Projective Geometry__\n",
    "As the methood using opencv is not exact (because the source and destination points have to be set manually), a geometry-based method for calculating the bev-image using coordinate system transformations will be discussed.\n",
    "\n",
    "The geometry-based method will be based on a simple camera model and will use the intrinsic and extrinsic matrices to compute the perspective transformation matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Camera Model__\n",
    "We will start by introducing the simple camera model we will use in this section.\n",
    "This model defines how a point $\\textbf{X}=(X,Y,Z)$ in the world frame gets transformed to a point $\\textbf{x}_{cam}=(x_c,y_c)$ on the camera image. \n",
    "\n",
    "This model uses intrinsic and extrinsic parameters of the camera. \n",
    "\n",
    "#### __Transformation from the world frame to the camera frame__\n",
    "Here we **assume** that the **world frame** is the **vehicle's main frame** since our goal is to produce a **vehicle-centered** BEV image or semantic grid map. The main vehicle's coordinate frame is often referred to as the __base link__. It specifies the coordinates of the vehicle on the map.\n",
    "\n",
    "We define a homogeneous transformation **E** that transforms points in the world frame to the camera frame (still three-dimensional) as shown in the follwing equation: \n",
    "\n",
    "$$ \\large X_{cam} = E X $$\n",
    "\n",
    "where **X** represents the homogeneous coordinate of the (three-dimensinal) point in the world frame (here also the vehicle's main frame) and **X<sub>cam</sub>** represents the homogeneous coordinate of the (three-dimensinal) point in the camera frame. \n",
    "\n",
    "\n",
    "This homogeneous transformation matrix incorporates the **extrinsic parameters** of the camera. \n",
    "We will refer to this matrix as the **extrinsic matrix**.\n",
    "It depends on the pose of the camera with respect to a reference frame. In our case, the extrinsic matrix transforms the main vehicle's coordinate frame to the camera coordinate frame. \n",
    "\n",
    "\n",
    "**E** can be composed of a 3x3 rotation matrix **R** and a 3x1 translation vector **t**.      \n",
    "\n",
    "$$ \\large E = \\begin{bmatrix} R & t \\\\ 0^T & 1\\end{bmatrix} = \\begin{bmatrix} R & -R \\tilde{C} \\\\ 0^T & 1\\end{bmatrix}$$\n",
    "\n",
    "Where $\\tilde{C}$ is the coordinate of the frame in the vehilce's main frame. \n",
    "\n",
    "Now that we can map points to the camera frame, we will project them into the image plane.\n",
    "\n",
    "#### __Transformation from the camera frame to the image plane__\n",
    "We define a projective transformation **K** that transforms three-dimensional points in the camera frame coordinates to two-dimensional points in the image coordinates. This transformation can also be defined as a matrix, which depends on the parameters of the camera, like the focal length $f$ and the principal point $p$. (Here we ignore the lens distortion).\n",
    "We will refer to this matrix as the **intrinsic matrix**. \n",
    "The homogeneous coordinates of a three-dimentional point in the camera frame are mapped to the homogeneous coordinates of two-dimensional point in the image plane according to the following equation:\n",
    "\n",
    "$$ \\large x_{cam} = K [I|0] X_{cam} = K \\begin{bmatrix} 1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0\\end{bmatrix} X_{cam} $$\n",
    "\n",
    "\n",
    "where **X<sub>cam</sub>** is the homogeneous coordinates of the (three-dimensinal) point in the camera frame and **x<sub>cam</sub>** is the homogeneous coordinate of the (two-dimentional) point in the camera plane.\n",
    "\n",
    "**K** can be written as follows:\n",
    "\n",
    "$$ \\large K = \\begin{bmatrix} f_x & 0 & p_x \\\\ 0 & f_y & p_y \\\\ 0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "### Camera projection matrix\n",
    "Using the extrinsic and intrinsic transformation matrices we can define the transformation that maps point in the vehicle's coordinates system to the image plane coordinate system, according to the following equation:\n",
    "\n",
    "$$ \\large x_{cam} = P X$$\n",
    "\n",
    "The projection matrix **P** can be calculated by multiplying **K** and **E**:  \n",
    "\n",
    "$$ \\large P = K \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0\\end{bmatrix} E $$\n",
    "\n",
    "The matrix is $\\large [I | 0]$ is needed since we only need the __inhomogeneous__ coordinates of the point __X__ in the world frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: get the camera intrinsic matrix__\n",
    "Camera parameters are can be estimated by applying camera calibration.\n",
    "Here the camera calibration was already applied and the parameters are provided in an external file. \n",
    "The goal of this task is to extract these parameters from an external file.\n",
    "\n",
    "Your task here is to read the intrinsic matrix from a json file.\n",
    "For this purpose, you will implement a function that gets a path to a JSON file and returns a dictionary containing the intrinsics of all cameras in the file as **NumPy arrays**. \n",
    "Complete the function `get_intrinsics()` by replacing the `None` placeholders.\n",
    "#### __Hints:__\n",
    "- Inspect the file `intrinsics.json` provided under `ipm_assets/cameras/intrinsics.json`.\n",
    "- Use python `open()` to read the json file. Refer to the [documentation](https://docs.python.org/3/library/functions.html#open).  \n",
    "- Use the function `json.load()`. Refer to the [documentation](https://docs.python.org/3/library/json.html).\n",
    "- after reading the file you should get dictionnary that contains the intrinsic matrix of **8 cameras** mounted on a vehicle. In the json files, the cameras are named: **('vr_1', 'vl_1', 'hr_1', 'hl_1', 'vr_2', 'vl_2', 'hr_2', 'hl_2')**\n",
    "- Note that loaded matrices are saved as python lists and need to be converted to NumPy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intrinsics(f_path=\"ipm_assets/cameras/intrinsics.json\"):\n",
    "    ### START CODE HERE ###\n",
    "    file = None\n",
    "    intrinsics_dict = None\n",
    "    for camera_n, intrinsic_matrix in intrinsics_dict.items():\n",
    "        intrinsics_dict[None] = np.array(None)\n",
    "    ### END CODE HERE ###\n",
    "    return intrinsics_dict\n",
    "\n",
    "intrinsics_dict = get_intrinsics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary we loaded contains the intrinsic matrices of 8 different cameras. Let's print the intrinsic matrix of the **front right camera**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intrinsics of the first front facing camera (vr_1)\n",
    "K = np.array(intrinsics_dict['vr_1'])\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: calculate the camera extrinsic matrix__\n",
    "The goal of this task is to compute the extrinsic matrix of the camera.\n",
    "\n",
    "Your task here is to read the extrinsic camera parameters from a JSON file.\n",
    "The provided JSON files contain extrinsic information about multiple camera-mounted vehicles. \n",
    "This information consists in the translation of each camera frame w.r.t the vehicle frame.\n",
    "It also contains the orientation of each camera frame w.r.t the vehicle frame expressed in roll, pitch yaw angles.\n",
    "\n",
    "The **position** and **orientation** of the camera need to be converted into a homogeneous transformation. \n",
    "\n",
    "\n",
    "The goal here is to implement a function that gets a path to a JSON file and returns a dictionary containing the homogeneous transformations describing the extrinsic parameters of all cameras in the file as NumPy arrays.\n",
    "\n",
    "Replace the `None` placeholders with your code.\n",
    "#### __Hints:__\n",
    "- Read the translation vector (saved in the dictionary as 'translation') and convert it to a NumPy array.\n",
    "- Read (roll, pitch, yaw) (saved in the dictionary as 'rotation_rpy').\n",
    "- The representation (`roll`, `pitch`, `yaw`) is equivalent to (3x3)-rotation matrices performing the following operations in order:\n",
    "    - a rotation around the x-axis with the roll angle\n",
    "    - a rotation around the y-axis with the pitch angle\n",
    "    - a rotation around the z-axis with the yaw angle\n",
    "- Combine into a homogeneous transform (4x4)\n",
    "    - use [`numpy.column_stack()`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html) and [`numpy.row_stack()`](https://numpy.org/devdocs/reference/generated/numpy.row_stack.html)\n",
    "    - Start by performing a column-wise stacking of the rotation matrix and the translation vector. The result should be a 3x4 matrix,\n",
    "    - then perform a row-wise stacking of the resulting 3x4 matrix and the array `np.array([0., 0., 0., 1.])`. The result should be a 4x4 matrix. \n",
    "    - The result should be a dictionary that contains camera names as keys and their corresponding extrinsic matrices as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def get_extrinsics(f_path=\"ipm_assets/cameras/extrinsics_rpy.json\"):\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # read json file\n",
    "    file = None\n",
    "    extrinsics_rpy_dict = None\n",
    "\n",
    "    # extrinsic dict (here we will save the extrinsic matrix for each camera)\n",
    "    extrinsics_dict = dict()\n",
    "    \n",
    "    for camera_n, extrinsic_params in extrinsics_rpy_dict.items():\n",
    "        # extract translation parameter\n",
    "        t = None\n",
    "        \n",
    "        # get roll, pitch and yaw for the camera 'cmera_n'\n",
    "        roll, pitch, yaw = None\n",
    "        # compute rotation matrix\n",
    "        Rz = None\n",
    "        Ry = None\n",
    "        Rx = None\n",
    "\n",
    "        R = Rz.dot(Ry.dot(Rx))\n",
    "\n",
    "        \n",
    "        # combine translation (3x1) and roatation matrix (3x3) into a 4x4 homogeneous transform\n",
    "        transform = None\n",
    "        # add 1 row ([0., 0., 0., 1.]) to complete the transform \n",
    "        transform = None\n",
    "        extrinsics_dict[camera_n] = None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return extrinsics_dict\n",
    "\n",
    "extrinsics_dict = get_extrinsics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the extrinsic matrix of the **front right camera**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extrinsics of the first front facing camera (vr_1)\n",
    "E = extrinsics_dict['vr_1']\n",
    "E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can now build the complete camera projection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: Calculate the projection matrix__\n",
    "\n",
    "Calculate the camera projection matrix of the front right camera by replacing the `None` placeholder with your code.\n",
    "#### __hints__:\n",
    "\n",
    "- The output should be a 3x4 matrix\n",
    "- The extrinsic matrix is a 4x4 matrix where the intrinsic matrix is a 3x3 matrix, so don't forget the $[I|0]$ matrix\n",
    "- Remember that we already saved the intrinsic and extrinsic matrices of the front right camera (in `K` and `E`) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "P = None\n",
    "### END CODE HERE ###\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: Mapping from  the road (BEV image) coordinates to the vehicle's coordinates__  \n",
    "\n",
    "Now we will define the mapping **M** from (two-dimensional) BEV image plane points (in homogeneous road coordinates) to (three-dimensional) world points (in homogeneous vehicle frame coordinates). \n",
    "In its simplest form the mapping would have the following equation:\n",
    "\n",
    "$$\\large  M_{\\text{2Dto3D}} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "The equaion for $M_{\\text{2Dto3D}}$ is based on the assumption that *z<sub>r</sub> = 0* (all points on the road plane have the height zero).\n",
    "\n",
    "In addition to *z<sub>r</sub> = 0* , this equation also requires that the coordinate system of the road and the vehicle are colocated and have the same orientation. \n",
    "Since not all of these conditions are satisfied, we have to adjust M.\n",
    "The figure below shows that the two frames are not colocated and don't have the same orientation.\n",
    "\n",
    "![Expected Output](ipm_assets/images/road_to_vehicle.png \"OpenCV cooridnates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "Let's first define some parameters for the mapping **M**. \n",
    "- `px_per_m` is the resolution and sets the number of pixels used to draw 1m. This resolution is used vertically and horizontally. 1px corresponds to 1mÂ².\n",
    "- `output_width` and `output_height` are the dimensions of the BEV image.\n",
    "- `shift_x` and `shift_y` specify with how much the origin of the vehicle frame is shifted w.r.t the road frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for ipm\n",
    "# output resolution\n",
    "px_per_m = 10 # number of pixels per meter\n",
    "# output size\n",
    "output_width = 798\n",
    "output_height = 400\n",
    "# shift to center of output image\n",
    "shift_x = output_width/ 2.0 \n",
    "shift_y = output_height / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Add shift operation__:\n",
    "\n",
    "The first adjustment you will have to make to the mapping **M**, from the road coordinates frame to the vehicles coordinates frame, would be **shifting** the coordinates frame of the road to the coordinates frame of the vehicle. \n",
    "\n",
    "For this pupose, **replace** the `None` placeholder with your code to define a homogeneous transformation matrix that performs a translation with (x<sub>shift</sub>, y<sub>shift</sub>) w.r.t the road frame.\n",
    "\n",
    "##### __hints__:\n",
    "- The homogeneous matrix incorporating a translation with (x<sub>shift</sub>, y<sub>shift</sub>) can be written as \n",
    "\n",
    "\n",
    "$$ \\large M_{\\text{shift}} = \\begin{bmatrix} 1 & 0 & x_{\\text{shift}}\\\\ \n",
    "0 & 1 & y_{\\text{shift}} \\\\ \n",
    "0 & 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "M_shift = None\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Change the direction of the y-axis__:\n",
    "\n",
    "The second adjustment you will have to make to the mapping **M**, would be **mirroring** the coordinates frame of the road on the x-axis, such that the y-axis of both frames match. \n",
    "\n",
    "For this purpose, **replace** the `None` placeholder with your code to define a homogeneous transformation matrix that performs the mirroring.\n",
    "\n",
    "##### __hints__:\n",
    "- With what homogeneous matrix does a homogeneous point **x** = (x,y,1) need to change the sign of y \n",
    "\n",
    "$$  \\large \\begin{bmatrix} x\\\\ \n",
    "-y\\\\ \n",
    "1\\end{bmatrix} = M_{\\text{direction}}\\begin{bmatrix} x\\\\ \n",
    "y\\\\ \n",
    "1\\end{bmatrix} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "M_direction = None\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Change the scaling of the y-axis__:\n",
    "\n",
    "The second adjustment you will have to make to the mapping **M**, would be **scaling** the coordinates in the road frame. This scaling depends on the specified resolution for the transformation.\n",
    "\n",
    "For this purpose, **replace** the `None` placeholder with your code to define a homogeneous transformation matrix that performs the scaling.\n",
    "\n",
    "##### __hints__:\n",
    "- A homogeneous transformation that performs scaling of the inputs with the value **a** can be written as:\n",
    "$$ \\large M_{\\text{scale}} = \\begin{bmatrix} a & 0 & 0\\\\ \n",
    "0 & a & 0 \\\\ \n",
    "0 & 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "- Remember that the inputs (coordinates in the BEV image) have the unit pixel and the (coordinates in the road) the unit meter \n",
    "- Use `px_per_m` for scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "M_scale = None\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Adjusted transformation M:__\n",
    "The transformation new M combines `M_2Dto3D` and the **shift**, **mirroring** and **scaling** operations needed for the adjustment.\n",
    "\n",
    "**replace** the `None` placeholder with your code to define the adjusted transformation matrix **M**.\n",
    "\n",
    "#### __hints__\n",
    "- (One) valid order of the operations: shift (`M_shift`), mirroring(`M_direction`), scaling(`M_scale`) than mapping (`M_2Dto3D`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# direct mapping (assuming z=0)\n",
    "\n",
    "M_2Dto3D = None\n",
    "# adjusted\n",
    "M = None\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the needed parts to build a mapping between camera images and the BEV image, we can compute the **inverse perspective mapping matrix**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: calculate the inverse perspective mapping matrix__\n",
    "Calcuate the mapping from the road image to the front camera image, then calculate the inverse matrix, which transforms images from the camera to the road (or top-down) view.\n",
    "\n",
    "Replace the `None` placeholder with your code.\n",
    "#### __hints:__\n",
    "- the mapping from the road image to the front camera image is defined as follows (we need the **inverse**)\n",
    "\n",
    "$$ \\large x_{cam} = P X = K [I|0] E X= K[R|t] M x_r$$\n",
    "\n",
    "- __X__ is the coordinate of a point in the vehicle frame\n",
    "- __x<sub>r</sub>__ is the coordinate of the same point in the road frame\n",
    "- __x<sub>cam</sub>__ is the coordinate of the same point in image coordinates\n",
    "- use `numpy.linalg.inv()` (dcoumentation [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) to invert the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "M_ipm = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(M_ipm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: Apply the inverse perspective mapping matrix on one image__\n",
    "Here you will use the matrix M_ipm to apply the perspective mapping.\n",
    "You can use cv2.warpPerspective() which takes as input an image and a mapping matrix (3x3). \n",
    "This function applies the homography matrix on every pixel in the input image.\n",
    "For every position (x<sub>c</sub>,y<sub>c</sub>) in the input image, (x<sub>r</sub>,y<sub>r</sub>) is calculated.\n",
    "cv2.warpPerspective(), also applies interpolation.\n",
    "\n",
    "Replace the `None` placeholder with your code.\n",
    "\n",
    "#### __hints:__\n",
    "- read the image from **\"ipm_assets/images/vr_1.png\"**. Use [`cv2.imread()`](https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html)\n",
    "- use OpenCV's perspective warp function: `cv2.warpPerspective()`\n",
    "- expected output \n",
    "\n",
    "![Expected Output](ipm_assets/images/ipm_invalid.png \"ipm output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# read image\n",
    "image = None\n",
    "\n",
    "# use M_ipm to perform the perspective mapping\n",
    "img_out = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "show_image(img_out, \"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping we designed doesn't hold for pixels above the ground level.\n",
    "That is why buildings are mapped incorrectly.  \n",
    "\n",
    "You can solve this by simply masking the upper part of the input image (which doesn't show the road).\n",
    "\n",
    "Replace the `None` placeholders with your code.\n",
    "\n",
    "\n",
    "#### hints:\n",
    "- Define a zero matrix (numpy array) **mask**, that has the height and width of the input image. Use **dtype=\"uint8\"**\n",
    "- Draw a rectangle on the mask that has the color white=255 and covers the part of the image that we don't want to mask (lower half)\n",
    "- Use **cv2.bitwise_and(image, image, mask=mask)** to apply the mask on the input\n",
    "- Expected output: \n",
    "\n",
    "![Expected Output](ipm_assets/images/cropped_image.png \"Cropped image\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# extract the height and width of the input image \n",
    "input_img_height, input_img_width, _ = None\n",
    "# intialize the mask with np.zeros  \n",
    "mask = None\n",
    "# rectangle for the region of intrest\n",
    "cv2.rectangle(mask, None, None, None,  255, -1)\n",
    "# apply mask\n",
    "image = cv2.bitwise_and(image, image, mask=None)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# show the image\n",
    "show_image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: Putting everything together__\n",
    "\n",
    "In order to avoid doing the whole process of calculating the IPM matrix manually for different cameras, we can put everything we did into one function that takes as inputs:\n",
    "\n",
    "- image\n",
    "- extrinsic matrix of the camera \n",
    "- intrinsic matrix of the camera\n",
    "- a config dictionary (output heigh, width, resolution, etc.)\n",
    "\n",
    "and outputs:\n",
    "- BEV image\n",
    "\n",
    "Replace the `None` placeholders with your code.\n",
    "\n",
    "##### __Hints:__ \n",
    "- don't forget to mask the upper half of every input image\n",
    "- expected output:\n",
    "\n",
    "![Expected Output](ipm_assets/images/output_expected.png \"Cropped image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have loaded extrinsics_dict, intrinsics_dict\n",
    "E = extrinsics_dict['vr_1']\n",
    "K = intrinsics_dict['vr_1']\n",
    "config = {}\n",
    "config[\"px_per_m\"] = 10 # number of pixels per meter\n",
    "config[\"output_width\"] = 798\n",
    "config[\"output_height\"] = 400\n",
    "# shift to center of output image\n",
    "config[\"shift_x\"] = config[\"output_width\"] / 2.0 \n",
    "config[\"shift_y\"] = config[\"output_height\"] /2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ipm(image, E, K, config):\n",
    "    # parameters for ipm\n",
    "    # output resolution\n",
    "    px_per_m = config[\"px_per_m\"] \n",
    "    # output size\n",
    "    width = config[\"output_width\"]\n",
    "    height = config[\"output_height\"]\n",
    "    # shift to center of the left edge of output image\n",
    "    shift_x = config[\"shift_x\"]\n",
    "    shift_y = config[\"shift_y\"]\n",
    "    \n",
    "    \n",
    "    input_img_height, input_img_width, _ = image.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    mask = None\n",
    "    cv2.rectangle(None, None, None,  255, -1)\n",
    "    image = None\n",
    "    \n",
    "    # define matrix that maps from the road frame to the vehicle frame\n",
    "    \n",
    "    M_2Dto3D = None\n",
    "    M_direction = None\n",
    "    M_shift = None\n",
    "    M_scale = None\n",
    "    \n",
    "    M = None\n",
    "    \n",
    "    # define projection matrix\n",
    "    P = None\n",
    "    \n",
    "    M_ipm = None\n",
    "    \n",
    "    \n",
    "    img_out = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "img = cv2.imread(\"ipm_assets/images/vr_1.png\")\n",
    "img = img[:, :, :]\n",
    "\n",
    "print(img.shape)\n",
    "# show image \n",
    "show_image(img, \"Original Image\")\n",
    "\n",
    "# apply ipm\n",
    "img_out2 = apply_ipm(img, E, K, config)\n",
    "# show output image \n",
    "show_image(img_out2, \"Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a single camera doesn't result in the 360Â° BEV. We can apply IPM on images from cameras facing different directions and stitch the result in BEV.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task: stitch multiple images in BEV__\n",
    "In this task, you will use images from 8 different cameras.\n",
    "\n",
    "You will load multiple images from a folder and use the extrinsic and intrinsic camera parameters loaded from the JSON file and use them to apply inverse perspective mapping on the images. \n",
    "\n",
    "You will implement a simple stitching strategy: simply add the images from the different cameras in the BEV.  \n",
    "Replace the `None` placeholders with your code.\n",
    "\n",
    "\n",
    "#### __Hints:__\n",
    "- Save images as a dictionay (e.g. **images_dict = {'vr_1': ...}**) with the same key used for the camera parameters\n",
    "- Create a zeros numpy array, that will contain the total BEV image, with the shape : `config['output_height'], config['output_width'],3)`\n",
    "- Iterate over all images: (e.g. for image_key, image_val in images_dict.items() ...)\n",
    "- Add the images to the total image **only at the position that are still = [0, 0, 0 ]** (black pixels)\n",
    "- Black pixels in an image can be isolated using \"**image[image==(0,0,0)]**\"\n",
    "- expected output:\n",
    "\n",
    "![Expected Output](ipm_assets/images/rgb_bev_360.png \"RGB BEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from folder\n",
    "images_directory = \"ipm_assets/cameras/images\"\n",
    "images = {}\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_pth = os.path.join(images_directory, filename)\n",
    "        #print(image_pth)\n",
    "        images[filename.split('.')[0]] = cv2.imread(image_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply bev and add images \n",
    "def apply_ipm_and_stitch_images(images, config, extrinsics_dict, intrinsics_dict):\n",
    "    ### START CODE HERE ###\n",
    "    bev_total_img = None\n",
    "    for n, image in images.items():\n",
    "        output_image = None\n",
    "        #show_image(output_image, n)\n",
    "        bev_total_img[bev_total_img==(0,0,0)] = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    show_image(bev_total_img)                     \n",
    "        \n",
    "apply_ipm_and_stitch_images(images, config, extrinsics_dict, intrinsics_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## __Task: Apply to semantically segmented images__\n",
    "The same way we applied IPM on RGB images, we can **apply it on semantically segmented images**. The resulting **BEV images will also be semantically segmented**.\n",
    "\n",
    "The task here is to perform camera-based semantic grid mapping by using IPM on semantically segmented images.\n",
    "Here you will use the same approach we used for normal camera images.\n",
    "\n",
    "Run the two next cells, then replace the `None` placeholders with your code.\n",
    "\n",
    "#### __hints__:\n",
    "- expected output:\n",
    "\n",
    "![Expected Output](ipm_assets/images/sem_bev_360.png \"RGB BEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get extrinsic matrices dictionnary\n",
    "extrinsics_dict = get_extrinsics(f_path=\"ipm_assets/carla_data/extrinsics_rpy.json\")\n",
    "\n",
    "intrinsics_dict = get_intrinsics(f_path=\"ipm_assets/carla_data/intrinsics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from folder\n",
    "images_directory = \"ipm_assets/carla_data/frames\"\n",
    "images = {}\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_pth = os.path.join(images_directory, filename)\n",
    "        #print(image_pth)\n",
    "        images[filename.split('.')[0]] = cv2.imread(image_pth)\n",
    "# list all images names in the dictionary \"images\"       \n",
    "print(images.keys())\n",
    "\n",
    "# show front right image \n",
    "show_image(images['front_right_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###        \n",
    "\n",
    "apply_ipm_and_stitch_images(None, None, None, None)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Wrap Up__\n",
    "\n",
    "- You learned how to apply inverse perspective mapping using opencv.\n",
    "- You learned how to apply inverse perspective mapping using the camera model and coordinate transformations.\n",
    "- You learned how to stitch multiple images in bird-eye-view.\n",
    "- You learned how to compute a geometry-based semantic grid mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright 2022 Institute for Automotive Engineering of RWTH Aachen University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
