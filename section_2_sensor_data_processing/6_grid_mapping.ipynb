{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header Image](../assets/header_image.png \"Header Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Point Cloud Occupancy Grid Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will get in touch with a deep learning pipeline that is used to train a model, which creates occupancy grid maps from 3D lidar point clouds. In the last exercise, you have created a geometry-based algorithm that evaluates each point in a point cloud to create an occupancy grid map. In a **deep learning-based approach**, the task is not to develop the grid mapping algorithm itself but to **create a training pipeline**. This means that the **input and label data has to be loaded and prepared** and a **neural network architecture has to be chosen** so that the neural network is able to learn how to create occupancy grid maps from 3D point clouds while trained with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"grid_mapping/input-label.png\" alt=\"input and label visualization\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will walk through the following steps:\n",
    "\n",
    "- Loading datasets for training, validating and testing the model\n",
    "- Building a data pipeline including augmentation\n",
    "- Creating a TensorFlow model architecture\n",
    "- Training and validating the model with synthetic data and analyzing the loss\n",
    "- Testing the model on synthetic and real-world data and analyzing the prediction results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Our data pipeline uses again the [modified version of the point_pillars package](https://github.com/ika-rwth-aachen/PointPillars) to structure 3D point clouds in pillars that are then processed by the neural network. This is described in the [original PointPillars publication](https://arxiv.org/abs/1812.05784). We already installed the `point_pillars` python package and setup everything within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "In the beginning, we will set some parameters that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_points_per_pillar = 100\n",
    "max_pillars = 10000\n",
    "number_features = 9\n",
    "number_channels = 64\n",
    "step_x_size = 0.16\n",
    "step_y_size = 0.16\n",
    "x_min = -40.96\n",
    "x_max = 40.96\n",
    "y_min = -28.16\n",
    "y_max = 28.16\n",
    "z_min = -3.0\n",
    "z_max = 1.0,\n",
    "min_distance = 3.0\n",
    "intensity_threshold = 100\n",
    "print_time = False\n",
    "label_resize_x = 256\n",
    "label_resize_y = 176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets\n",
    "We will load datasets consisting of lidar point clouds (as inputs) and occupancy grid maps (as labels) using [TensorFlow Datasets](https://www.tensorflow.org/datasets). The dataset is available as ZIP archive containing point clouds stored as [PCD files](https://pointclouds.org/documentation/tutorials/pcd_file_format.html) and occupancy grid maps stored as PNG image files. The Tensorflow Dataset description ([`evilog_2021.py`](grid_mapping/evilog_2021.py)) contains instructions how these files can be downloaded and converted to [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html), which can be processed by TensorFlow. This is done automatically, when `tfds.load()` is executed. Subsequent runs will reuse the prepared data, which speeds up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import grid_mapping.evilog_2021\n",
    "\n",
    "dataset_train = tfds.load('evilog2021/demo', split='train', data_dir=\"tensorflow_datasets\")\n",
    "dataset_valid = tfds.load('evilog2021/demo', split='valid', data_dir=\"tensorflow_datasets\")\n",
    "dataset_test = tfds.load('evilog2021/demo', split='test', data_dir=\"tensorflow_datasets\")\n",
    "dataset_real = tfds.load('evilog2021/demo', split='real', data_dir=\"tensorflow_datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time reasons, we will use a small demo dataset comprising 100 training, 50 validation and 50 test samples that were created using simulation. Additionally, there are 50 real point clouds included, which were recorded with the ika's research vehicle in urban traffic. Wait until all four datasets are prepared. You can inspect the downloaded files at `tensorflow_datasets/downloads/extracted/`.\n",
    "\n",
    "#### Task 1: Data Shape\n",
    "\n",
    "Now have a look at the shape of one input and label sample. Can you guess how the data is structured? Complete the following code to get the `x`, `y`, and `z` coordinate of the first point in the input point cloud. Note that you can append `.numpy()` to a `tf.Tensor` to convert the tensor to a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset_train.take(1):\n",
    "    print(\"grid_map shape: \" + str(sample[\"grid_map\"].shape))\n",
    "    print(\"point_cloud shape: \" + str(sample[\"point_cloud\"].shape))\n",
    "    # TASK 1: Get the x, y and z coordinate (as scalar) from the first point in the point cloud\n",
    "    ### START CODE HERE ###\n",
    "    x = 0\n",
    "    y = 0\n",
    "    z = 0\n",
    "    ### END CODE HERE ###\n",
    "    print(\"x = \" + str(x) + \", y = \" + str(y) + \", z = \" + str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "grid_map shape: (512, 352, 2)\n",
    "point_cloud shape: (27417, 4)\n",
    "x = 18.384949, y = -8.262523, z = 5.4008617\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Data Pipeline\n",
    "\n",
    "With our datasets prepared, we will now build the data pipeline that is executed during the training to feed samples into TensorFlow's optimization algorithm. At first, we create a function for data preprocessing. Here, the raw point clouds are converted to pillar tensors and the data is augmented.\n",
    "\n",
    "#### Task 2: Data Augmentation\n",
    "\n",
    "We want to use data augmentation to increase the diversity of the training dataset. One such augmentation method is randomly rotating input and label by the same angle. We can use a TensorFlow method to rotate the grid maps (as they are like images) but there is no method to rotate 3D point clouds. Complete the method `augmentSample` below so that the point cloud is rotated by the same angle as the grid map.\n",
    "\n",
    "Hints: `point_cloud` has the shape `[N, 4]`, i.e. a list of points with properties `[x, y, z, intensity]`. Create a rotation matrix to rotate the point cloud in x-y-plane. The final command applies the rotation to all points of the point cloud at once (`@` is matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from point_pillars import createPillars\n",
    "import random\n",
    "from scipy.ndimage import rotate\n",
    "import math\n",
    "import numpy as np\n",
    "from grid_mapping import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# This method augments one training sample. It is called by `preprocessSample`\n",
    "def augmentSample(point_cloud, grid_map):\n",
    "    angle = random.uniform(-math.pi, math.pi)\n",
    "    grid_map = rotate(grid_map, np.degrees(angle), mode='nearest', order=0)\n",
    "\n",
    "    # TASK 2: Create a rotation matrix that rotates the point cloud according to the grid map\n",
    "    ### START CODE HERE ###\n",
    "    rotation_matrix = np.array( [[1.0, 1.0],\n",
    "                                 [1.0, 1.0]] )\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    point_cloud[:, 0:2] = (rotation_matrix @ point_cloud[:, 0:2].T).T\n",
    "\n",
    "    return point_cloud, grid_map\n",
    "\n",
    "\n",
    "# This method preprocesses the raw training sample before using it to optimize the model parameters\n",
    "def preprocessSample(input_, label_=None):\n",
    "    point_cloud = input_\n",
    "    grid_map = label_\n",
    "    # augment training sample (method above)\n",
    "    if grid_map is not None:\n",
    "        point_cloud, grid_map = augmentSample(point_cloud, grid_map)\n",
    "\n",
    "    if intensity_threshold is not None:\n",
    "            point_cloud[:, 3] = np.clip(point_cloud[:, 3] / intensity_threshold,\n",
    "                                        0.0,\n",
    "                                        1.0,\n",
    "                                        dtype=np.float32)\n",
    "\n",
    "    # convert list of points to tensor of pillars\n",
    "    pillars, voxels = tf.numpy_function(createPillars, [point_cloud, max_points_per_pillar, max_pillars, step_x_size, step_y_size,\n",
    "                                        x_min, x_max, y_min, y_max, z_min, z_max, print_time, min_distance], [np.float32, np.int32])\n",
    "\n",
    "    # resize the grid map to the same size as the network output\n",
    "    if grid_map is not None:\n",
    "        grid_map = tf.image.resize(grid_map, [label_resize_x, label_resize_y], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        return pillars, voxels, grid_map\n",
    "    else:\n",
    "        return pillars, voxels\n",
    "\n",
    "\n",
    "# this wrapper allows using numpy functions in `preprocessSample``\n",
    "def numpyWrapper(input_, label_=None):\n",
    "    if label_ is not None:\n",
    "            pillars, voxels, grid_map = tf.numpy_function(\n",
    "                func=preprocessSample,\n",
    "                inp=[input_, label_],\n",
    "                Tout=[tf.float32, tf.int32, tf.float32])\n",
    "    else:\n",
    "        pillars, voxels = tf.numpy_function(func=preprocessSample,\n",
    "                                            inp=[input_],\n",
    "                                            Tout=[tf.float32, tf.int32])\n",
    "    \n",
    "    # set Tensor shapes, as tf is unable to infer rank from py_function\n",
    "    # augmented lidar point is 9-dimensional\n",
    "    pillars.set_shape([1, None, None, 9])\n",
    "    voxels.set_shape([1, None, 3])\n",
    "    if label_ is not None:\n",
    "        grid_map.set_shape([None, None, 2])\n",
    "\n",
    "    # remove batch dim from input tensors, will be added by data pipeline\n",
    "    pillars = tf.squeeze(pillars, axis=0)\n",
    "    voxels = tf.squeeze(voxels, axis=0)\n",
    "\n",
    "    network_inputs = (pillars, voxels)\n",
    "\n",
    "    if label_ is not None:\n",
    "        network_labels = (grid_map)\n",
    "    else:\n",
    "        network_labels = None\n",
    "\n",
    "    return network_inputs, network_labels\n",
    "\n",
    "\n",
    "# Test augmentation with first sample in dataset\n",
    "for sample in dataset_train.take(1):\n",
    "    # augment input and label\n",
    "    rotated_pointcloud, rotated_grid_map = augmentSample(sample[\"point_cloud\"].numpy(), sample[\"grid_map\"].numpy())\n",
    "    \n",
    "    # store rotated 3D point cloud as top view image\n",
    "    image_point_cloud = utils.lidar_to_bird_view_img(rotated_pointcloud, x_min, x_max, y_min, y_max, step_x_size, step_y_size, intensity_threshold)\n",
    "    im = Image.fromarray(image_point_cloud)\n",
    "    im.save('grid_mapping/point_cloud.png')\n",
    "\n",
    "    # store rotated occupancy grid map as image\n",
    "    image_grid_map = utils.grid_map_to_img(rotated_grid_map)\n",
    "    im = Image.fromarray(image_grid_map.numpy())\n",
    "    im.save('grid_mapping/grid_map.png')\n",
    "\n",
    "    # display images below\n",
    "    rcParams['figure.figsize'] = 11,8\n",
    "    img_A = mpimg.imread('grid_mapping/point_cloud.png')\n",
    "    img_B = mpimg.imread('grid_mapping/grid_map.png')\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].set_title(\"Point Cloud\")\n",
    "    ax[0].imshow(img_A);\n",
    "    ax[1].set_title(\"Occupancy Grid Map\")\n",
    "    ax[1].imshow(img_B);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the data pipelines for the training and validation dataset. Each sample in the dataset consists of a point cloud and a grid map. The dataset is shuffled in training epoch and all samples are preprocessed using the `preprocessSample()` method above. Then, batches of multiple samples are created. Each batch will be used sequentially during the training in TensorFlow's optimization algorithm to improve the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipe_train = dataset_train.map(lambda ex: (ex[\"point_cloud\"], ex[\"grid_map\"]))\n",
    "datapipe_train = datapipe_train.shuffle(buffer_size= len(dataset_train), reshuffle_each_iteration = True)\n",
    "datapipe_train = datapipe_train.map(numpyWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "datapipe_train = datapipe_train.batch(batch_size, drop_remainder = True)\n",
    "datapipe_train = datapipe_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "datapipe_valid = dataset_valid.map(lambda ex: (ex[\"point_cloud\"], ex[\"grid_map\"]))\n",
    "datapipe_valid = datapipe_valid.shuffle(buffer_size= len(dataset_valid), reshuffle_each_iteration = True)\n",
    "datapipe_valid = datapipe_valid.map(numpyWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "datapipe_valid = datapipe_valid.batch(batch_size, drop_remainder = True)\n",
    "datapipe_valid = datapipe_valid.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "datapipe_test_syn = dataset_test.map(lambda ex: (ex[\"point_cloud\"],))\n",
    "datapipe_test_syn_raw = datapipe_test_syn.batch(1, drop_remainder = True)\n",
    "datapipe_test_syn = datapipe_test_syn.map(numpyWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "datapipe_test_syn = datapipe_test_syn.batch(1, drop_remainder = True)\n",
    "\n",
    "datapipe_test_real = dataset_real.map(lambda ex: (ex[\"point_cloud\"],))\n",
    "datapipe_test_real_raw = datapipe_test_real.batch(1, drop_remainder = True)\n",
    "datapipe_test_real = datapipe_test_real.map(numpyWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "datapipe_test_real = datapipe_test_real.batch(1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Network Architecture\n",
    "We need to define how the neural network is structured, e.g. how many layers, which kernel sizes, whether there are skip connections etc. As there is no perfect recipe for finding the best network architecture, we base our work on an architecutre that performs very well on a similar task, i.e. 3D lidar object detection. Thus we use a slightly modified version of PointPillar's Feature Net and CNN backbone. Have a look at the file [`point_pillars.py`](grid_mapping/point_pillars.py) to see our modifications.\n",
    "\n",
    "#### Task 3: Evidential Prediction Head\n",
    "\n",
    "As we do not want the model to predict 3D bounding boxes but occupancy grid maps, we removed the original preidction heads and replaced it with an \"Evidential Prediction Head\". This is just another 2D convolutional layer, which calculates a tensor with the same dimensions as the grid map to be predicted. It has 2 filters, hence two values are calculated for each cell (i.e. evidence for the cell being free and evidence for the cell being occupied). The predicted evidence shall be in the range $ ( 0, \\inf ) $. Which activation function should be used? Set the parameter in the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_mapping.point_pillars import getPointPillarsModel\n",
    "\n",
    "# Model architecture\n",
    "def getModel():\n",
    "    Xn = int((x_max - x_min) / step_x_size)\n",
    "    Yn = int((y_max - y_min) / step_y_size)\n",
    "\n",
    "    # Point Pillars Feature Net and CNN backbone\n",
    "    input_pillars, input_indices, output = getPointPillarsModel(\n",
    "        tuple([Xn, Yn]), int(max_pillars),\n",
    "        int(max_points_per_pillar), int(number_features),\n",
    "        int(number_channels))\n",
    "\n",
    "    # Evidential Prediction Head\n",
    "    # TASK 3: Set the correct activation function\n",
    "    ### START CODE HERE ###\n",
    "    prediction = tf.keras.layers.Conv2D(filters=2, kernel_size=(3, 3),\n",
    "                                        padding=\"same\",\n",
    "                                        name=\"ogm/conv2d\",\n",
    "                                        activation=None)(output)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model([input_pillars, input_indices],\n",
    "                                 [prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's optimization algorithm requires a **loss function** to calculate how well the network's prediction fits the label. The training goal is to minimize this loss function. We use a custom loss function which calculates Kullback-Leiber divergence between predictions and labels. Have a look at [this publication](https://ieeexplore.ieee.org/document/9575715) if you are interested in the mathematics behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network predicts evidences for the cell being free or occupied in the range [0, inf].\n",
    "# This method converts evidences into belief masses and an uncertainty value in the range [0, 1]  \n",
    "def evidences_to_masses(logits):\n",
    "    # convert evidences to parameters of a Dirichlet distribution\n",
    "    alpha = logits + tf.ones(tf.shape(logits))\n",
    "\n",
    "    # Dirichlet strength (sum alpha for all classes)\n",
    "    S = tf.reduce_sum(alpha, axis=-1, keepdims=True)\n",
    "\n",
    "    num_classes = tf.cast(tf.shape(logits)[-1], tf.dtypes.float32)\n",
    "\n",
    "    # uncertainty\n",
    "    u = num_classes / S\n",
    "    # belief masses\n",
    "    prob = logits / S\n",
    "\n",
    "    return prob, u, S, num_classes\n",
    "\n",
    "\n",
    "class ExpectedMeanSquaredError(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.epoch_num = tf.Variable(0.0)\n",
    "\n",
    "    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "\n",
    "        prob, _, S, num_evidential_classes = evidences_to_masses(y_pred)\n",
    "\n",
    "        loss = tf.math.add(\n",
    "            tf.reduce_sum((y_true - prob)**2, axis=-1, keepdims=True),\n",
    "            tf.reduce_sum(prob * (1 - prob) / (S + 1), axis=-1, keepdims=True))\n",
    "        alpha = y_pred * (1 - y_true) + 1\n",
    "        KL_reg = tf.minimum(1.0, tf.cast(self.epoch_num / 10,\n",
    "                                         tf.float32)) * self.kl_regularization(\n",
    "                                             alpha, num_evidential_classes)\n",
    "        loss = loss + KL_reg\n",
    "\n",
    "        # higher weight for loss on evidence for state \"occupied\" because it is underrepresented in training data\n",
    "        weight_occupied = 100\n",
    "        loss = tf.where(y_true[..., 1] > 0.5,\n",
    "                        tf.squeeze(loss * weight_occupied, axis=-1),\n",
    "                        tf.squeeze(loss, axis=-1))\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def kl_regularization(self, alpha, K):\n",
    "        beta = tf.ones_like(alpha)\n",
    "        S_alpha = tf.reduce_sum(alpha, axis=-1, keepdims=True)\n",
    "        KL = tf.math.add_n([\n",
    "            tf.reduce_sum((alpha - beta) *\n",
    "                          (tf.math.digamma(alpha) - tf.math.digamma(S_alpha)),\n",
    "                          axis=-1,\n",
    "                          keepdims=True),\n",
    "            tf.math.lgamma(S_alpha) -\n",
    "            tf.reduce_sum(tf.math.lgamma(alpha), axis=-1, keepdims=True),\n",
    "            tf.reduce_sum(tf.math.lgamma(beta), axis=-1, keepdims=True) -\n",
    "            tf.math.lgamma(tf.reduce_sum(beta, axis=-1, keepdims=True))\n",
    "        ])\n",
    "        return KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the network architecture and loss function defined, we can now compile the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=ExpectedMeanSquaredError())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Model: \"model\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "pillars/input (InputLayer)      [(None, 10000, 100,  0                                            \n",
    "__________________________________________________________________________________________________\n",
    "pillars/conv2d (Conv2D)         (None, 10000, 100, 6 576         pillars/input[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "pillars/batchnorm (BatchNormali (None, 10000, 100, 6 256         pillars/conv2d[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "pillars/indices (InputLayer)    [(None, 10000, 3)]   0                                            \n",
    "__________________________________________________________________________________________________\n",
    "pillars/relu (Activation)       (None, 10000, 100, 6 0           pillars/batchnorm[0][0]          \n",
    "__________________________________________________________________________________________________\n",
    "...\n",
    "__________________________________________________________________________________________________\n",
    "cnn/up3/bn (BatchNormalization) (None, 256, 176, 128 512         cnn/up3/conv2dt[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "cnn/concatenate (Concatenate)   (None, 256, 176, 384 0           cnn/up1/bn[0][0]                 \n",
    "                                                                 cnn/up2/bn[0][0]                 \n",
    "                                                                 cnn/up3/bn[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "ogm/conv2d (Conv2D)             (None, 256, 176, 2)  6914        cnn/concatenate[0][0]            \n",
    "==================================================================================================\n",
    "Total params: 4,741,058\n",
    "Trainable params: 4,735,042\n",
    "Non-trainable params: 6,016\n",
    "__________________________________________________________________________________________________\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "We will train the model for 5 epochs, i.e. the model sees every training sample 5 times during training. We add two callbacks that are executed regularly during the training. \n",
    "\n",
    "- The `model_checkpoint_callback` will save the current state of the trained model parameters (weights) after each epoch to a HDF5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf grid_mapping/checkpoints\n",
    "!mkdir -p grid_mapping/checkpoints\n",
    "\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='grid_mapping/checkpoints/weights.{epoch:02d}.hdf5',\n",
    "    save_weights_only=True)\n",
    "\n",
    "history = model.fit(datapipe_train,\n",
    "                    validation_data=datapipe_valid,\n",
    "                    epochs=5,\n",
    "                    callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the training is performed using the CPU (and not the GPU) on the jupyter server, this will take a while. So get a coffee and **analyze how the training and validation error develops during the training process**. After the training you can execute the cell below to check the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model on Synthetic Data\n",
    "We will now use the previously trained model to create occupancy grid maps from real-world lidar point clouds.\n",
    "\n",
    "#### Task 4: Model Selection\n",
    "\n",
    "Have a look at the graph of the epochs' training and validation error. **The weights of which epoch should we take?** Add the missing epoch number to the filename below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# TASK 4: Replace XX with the 2-digit epoch number of the saved model you want to use\n",
    "### START CODE HERE ###\n",
    "model.load_weights('grid_mapping/checkpoints/weights.05.hdf5')\n",
    "### END CODE HERE ###\n",
    "\n",
    "num_plots = 5\n",
    "i = 0\n",
    "fig, axs = plt.subplots(num_plots, 2, figsize=(20, 40))\n",
    "for sample, sample_raw in zip(tqdm(datapipe_test_syn), datapipe_test_syn_raw):\n",
    "    point_cloud_batch = sample[0]  # get first input batch from the traning pipeline (after `preprocessSample`)\n",
    "    point_cloud_raw = sample_raw[0][0]  # get first input point cloud directly from the dataset (without preprocessing)\n",
    "\n",
    "    # use trained model to predict grid map\n",
    "    prediction_batch = model(point_cloud_batch, training=False)\n",
    "    grid_map = prediction_batch[0]  # get first predicted grid map from batch\n",
    "\n",
    "    # convert predicted evidences to belief masses\n",
    "    prob, u, S, num_classes = evidences_to_masses(grid_map)\n",
    "\n",
    "    # convert point cloud and grid map to images\n",
    "    image_point_cloud = utils.lidar_to_bird_view_img(point_cloud_raw.numpy(), x_min, x_max, y_min, y_max, step_x_size, step_y_size, intensity_threshold)\n",
    "    image_grid_map = utils.grid_map_to_img(prob)\n",
    "\n",
    "    # plot images in the grid\n",
    "    axs[i, 0].imshow(tf.cast(image_point_cloud, dtype=tf.float32)/255.0)\n",
    "    axs[i, 0].set_title('Synthetic Input')\n",
    "    axs[i, 1].imshow(image_grid_map)\n",
    "    axs[i, 1].set_title('Synthetic Prediction')\n",
    "\n",
    "    i = i + 1\n",
    "    if i >= num_plots:\n",
    "        break\n",
    "\n",
    "# adjust layout and show plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the perfomance of the trained model. Are the predicted occupancy grid maps helpful for an automated driving function? Can you explain the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Pretrained Model on Real-World Data\n",
    "\n",
    "We have added a model that was trained on a dataset of 10.000 samples for 100 epochs and will use this now to predict occupancy grid maps from real-world lidar point clouds. You can see the results if you execute the cell below. Can you imagine how these occupancy grid maps can be useful for automated driving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('grid_mapping/weights.99.hdf5')\n",
    "\n",
    "num_plots = 5\n",
    "i = 0\n",
    "fig, axs = plt.subplots(num_plots, 2, figsize=(20, 40))\n",
    "for sample, sample_raw in zip(tqdm(datapipe_test_real), datapipe_test_real_raw):\n",
    "    point_cloud_batch = sample[0]\n",
    "    point_cloud_raw = sample_raw[0][0]\n",
    "\n",
    "    # use trained model to predict grid map\n",
    "    prediction_batch = model(point_cloud_batch, training=False)\n",
    "    grid_map = prediction_batch[0]\n",
    "  \n",
    "    # convert predicted evidences to belief masses\n",
    "    prob, u, S, num_classes = evidences_to_masses(grid_map)\n",
    "  \n",
    "    # convert point cloud and grid map to images\n",
    "    image_point_cloud = utils.lidar_to_bird_view_img(point_cloud_raw.numpy(), x_min, x_max, y_min, y_max, step_x_size, step_y_size, intensity_threshold)\n",
    "    image_grid_map = utils.grid_map_to_img(prob)\n",
    "    \n",
    "    # plot images in the grid\n",
    "    axs[i, 0].imshow(tf.cast(image_point_cloud, dtype=tf.float32)/255.0)\n",
    "    axs[i, 0].set_title('Synthetic Input')\n",
    "    axs[i, 1].imshow(image_grid_map)\n",
    "    axs[i, 1].set_title('Synthetic Prediction')\n",
    "\n",
    "    i = i + 1\n",
    "    if i >= num_plots:\n",
    "        break\n",
    "\n",
    "# adjust layout and show plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Up\n",
    "\n",
    "In this notebook, you learned how to\n",
    "\n",
    "- load different datasets using the *TensorFlow Datasets* (tfds) API,\n",
    "- build a data pipline including augmentation for the task of occupancy grid mapping,\n",
    "- adapt the PointPillars network architecture to the new task of occupancy grid mapping,\n",
    "- train and evaluate the model's performance on synthetic and on real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@INPROCEEDINGS{9575715,\n",
    "  author={van Kempen, Raphael and Lampe, Bastian and Woopen, Timo and Eckstein, Lutz},\n",
    "  booktitle={2021 IEEE Intelligent Vehicles Symposium (IV)}, \n",
    "  title={A Simulation-based End-to-End Learning Framework for Evidential Occupancy Grid Mapping}, \n",
    "  year={2021},\n",
    "  pages={934-939},\n",
    "  doi={10.1109/IV48863.2021.9575715}}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@article{LAN2019,\n",
    "      title={PointPillars: Fast Encoders for Object Detection from Point Clouds}, \n",
    "      author={Alex H. Lang and Sourabh Vora and Holger Caesar and Lubing Zhou and Jiong Yang and Oscar Beijbom},\n",
    "      year={2019}\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@misc{TYA2021,\n",
    "  author = {tyagi-iiitv},\n",
    "  title = {PointPillars Tensorflow},\n",
    "  url = {https://github.com/tyagi-iiitv/PointPillars},\n",
    "  year = {2021}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright 2022 Institute for Automotive Engineering of RWTH Aachen University."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
